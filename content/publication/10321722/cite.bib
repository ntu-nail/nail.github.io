@article{10321722,
 abstract = { Vertical Federated Learning (VFL) enables multiple data owners, each holding a different subset of features about a largely overlapping set of data samples, to collaboratively train a global model. The quality of data owners’ local features affects the performance of the VFL model, which makes feature selection vitally important. However, existing feature selection methods for VFL either assume the availability of prior knowledge on the number of noisy features or prior knowledge on the post-training threshold of useful features to be selected, making them unsuitable for practical applications. To bridge this gap, we propose the Federated Stochastic Dual-Gate based Feature Selection (FedSDG-FS) approach. It consists of a Gaussian stochastic dual-gate to efficiently approximate the probability of a feature being selected. FedSDG-FS further designs a local embedding perturbation approach to achieve differential privacy for local training data. To reduce overhead, we propose a feature importance initialization method based on Gini impurity, which can accomplish its goals with only two parameter transmissions between the server and the clients. The enhanced version, FedSDG-FS++, protects the privacy for both the clients’ training data and the server's labels through Partially Homomorphic Encryption (PHE) without relying on a trusted third-party. Theoretically, we analyze the convergence rate, privacy guarantees and security analysis of our methods. Extensive experiments on both synthetic and real-world datasets show that FedSDG-FS and FedSDG-FS++ significantly outperform existing approaches in terms of achieving more accurate selection of high-quality features as well as improving VFL performance in a privacy-preserving manner. },
 address = {Los Alamitos, CA, USA},
 author = {Li, Anran and Huang, Jiahui and Jia, Ju and Peng, Hongyi and Zhang, Lan and Tuan, Luu Anh and Yu, Han and Li, Xiang-Yang},
 doi = {10.1109/TMC.2023.3333879},
 issn = {1558-0660},
 journal = { IEEE Transactions on Mobile Computing },
 keywords = {Feature extraction;Privacy;Data models;Training data;Noise measurement;Training;Mobile computing},
 month = {June},
 number = {06},
 pages = {7238-7255},
 publisher = {IEEE Computer Society},
 title = { Efficient and Privacy-Preserving Feature Importance-Based Vertical Federated Learning },
 url = {https://doi.ieeecomputersociety.org/10.1109/TMC.2023.3333879},
 volume = {23},
 year = {2024}
}
