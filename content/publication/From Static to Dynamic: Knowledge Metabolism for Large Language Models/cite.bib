@article{du_static_2024,
 abstract = {The immense parameter space of Large Language Models (LLMs) endows them with superior knowledge retention capabilities, allowing them to excel in a variety of natural language processing tasks. However, it also instigates difficulties in consistently tuning LLMs to incorporate the most recent knowledge, which may further lead LLMs to produce inaccurate and fabricated content. To alleviate this issue, we propose a knowledge metabolism framework for LLMs, which proactively sustains the credibility of knowledge through an auxiliary memory component and directly delivers pertinent knowledge for LLM inference, thereby suppressing hallucinations caused by obsolete internal knowledge during the LLM inference process. Benchmark experiments demonstrate DynaMindâ€™s effectiveness in overcoming this challenge. The code and demo of DynaMind are available at: https://github.com/Elfsong/DynaMind.},
 author = {Du, Mingzhe and Luu, Anh Tuan and Ji, Bin and Ng, See-Kiong},
 doi = {10.1609/aaai.v38i21.30564},
 file = {Du et al. - 2024 - From Static to Dynamic Knowledge Metabolism for L.pdf:files/7/Du et al. - 2024 - From Static to Dynamic Knowledge Metabolism for L.pdf:application/pdf},
 issn = {2374-3468, 2159-5399},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 language = {en},
 month = {March},
 number = {21},
 pages = {23784--23786},
 shorttitle = {From Static to Dynamic},
 title = {From Static to Dynamic: Knowledge Metabolism for Large Language Models},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/30564},
 urldate = {2024-04-09},
 volume = {38},
 year = {2024}
}
