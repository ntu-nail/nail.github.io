---
title: 'From Static to Dynamic: Knowledge Metabolism for Large Language Models'
authors:
- Mingzhe Du
- Anh Tuan Luu
- Bin Ji
- See-Kiong Ng
date: '2024-03-01'
publishDate: '2024-04-10T13:39:27.262445Z'
publication_types:
- article-journal
publication: '*Proceedings of the AAAI Conference on Artificial Intelligence*'
doi: 10.1609/aaai.v38i21.30564
abstract: 'The immense parameter space of Large Language Models (LLMs) endows them
  with superior knowledge retention capabilities, allowing them to excel in a variety
  of natural language processing tasks. However, it also instigates difficulties in
  consistently tuning LLMs to incorporate the most recent knowledge, which may further
  lead LLMs to produce inaccurate and fabricated content. To alleviate this issue,
  we propose a knowledge metabolism framework for LLMs, which proactively sustains
  the credibility of knowledge through an auxiliary memory component and directly
  delivers pertinent knowledge for LLM inference, thereby suppressing hallucinations
  caused by obsolete internal knowledge during the LLM inference process. Benchmark
  experiments demonstrate DynaMindâ€™s effectiveness in overcoming this challenge. The
  code and demo of DynaMind are available at: https://github.com/Elfsong/DynaMind.'
links:
- name: URL
  url: https://ojs.aaai.org/index.php/AAAI/article/view/30564
---
