---
title: 'Donâ€™t Forget Your Reward Values: Language Model Alignment via Value-based Calibration'
publication_types:
  - Conference
authors:
  - Xin Mao
  - Feng-Lin Li
  - Huimin Xu
  - Wei Zhang
  - Wang Chen
  - Luu_Anh_Tuan 

publication: "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
publication_short: EMNLP
abstract: 'While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based alignment methods as viable alternatives. This paper delves into existing order-based methods, unifying them into one framework and examining their inefficiencies in utilizing reward values. Building upon these findings, we propose a new Value-based Calibration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and diversity in different settings.'
draft: false
featured: false
tags:
  - EMNLP
date: '2024-11-01T00:00:00Z'
---
Link: https://aclanthology.org/2024.emnlp-main.976/