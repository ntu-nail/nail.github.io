---
title: Meta-optimized Angular Margin Contrastive Framework forÂ Video-Language Representation
  Learning

# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- Thong Nguyen
- Yi Bin
- Xiaobao Wu
- Xinshuai Dong
- Zhiyuan Hu
- Khoi Le
- Cong-Duy Nguyen
- See-Kiong Ng
- Luu Anh Tuan

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2025-01-01'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2025-01-06T12:05:10.008430Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- paper-conference

# Publication name and optional abbreviated publication name.
publication: '*Computer Vision -- ECCV 2024*'
publication_short: ''

doi: ''

abstract: Data quality stands at the forefront of deciding the effectiveness of video-language
  representation learning. However, video-text pairs in previous data typically do
  not align perfectly with each other, which might lead to video-language representations
  that do not accurately reflect cross-modal semantics. Moreover, previous data also
  possess an uneven distribution of concepts, thereby hampering the downstream performance
  across unpopular subjects. To address these problems, we propose a contrastive objective
  with a subtractive angular margin to regularize cross-modal representations in their
  effort to reach perfect similarity. Furthermore, to adapt to the non-uniform concept
  distribution, we propose a multi-layer perceptron (MLP)-parameterized weighting
  function that maps loss values to sample weights which enable dynamic adjustment
  of the model's focus throughout the training. With the training guided by a small
  amount of unbiased meta-data and augmented by video-text data generated by large
  vision-language model, we improve video-language representations and achieve superior
  performances on commonly used video question answering and text-video retrieval
  datasets.

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
---

Add the **full text** or **supplementary notes** for the publication here using Markdown formatting.
