@inproceedings{10.1007/978-3-031-72989-8_5,
 abstract = {Data quality stands at the forefront of deciding the effectiveness of video-language representation learning. However, video-text pairs in previous data typically do not align perfectly with each other, which might lead to video-language representations that do not accurately reflect cross-modal semantics. Moreover, previous data also possess an uneven distribution of concepts, thereby hampering the downstream performance across unpopular subjects. To address these problems, we propose a contrastive objective with a subtractive angular margin to regularize cross-modal representations in their effort to reach perfect similarity. Furthermore, to adapt to the non-uniform concept distribution, we propose a multi-layer perceptron (MLP)-parameterized weighting function that maps loss values to sample weights which enable dynamic adjustment of the model's focus throughout the training. With the training guided by a small amount of unbiased meta-data and augmented by video-text data generated by large vision-language model, we improve video-language representations and achieve superior performances on commonly used video question answering and text-video retrieval datasets.},
 address = {Cham},
 author = {Nguyen, Thong
and Bin, Yi
and Wu, Xiaobao
and Dong, Xinshuai
and Hu, Zhiyuan
and Le, Khoi
and Nguyen, Cong-Duy
and Ng, See-Kiong
and Tuan, Luu Anh},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Aleš
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, Gül},
 isbn = {978-3-031-72989-8},
 pages = {77--98},
 publisher = {Springer Nature Switzerland},
 title = {Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning},
 year = {2025}
}
