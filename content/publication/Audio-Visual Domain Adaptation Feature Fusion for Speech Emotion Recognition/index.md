---
title: Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition
publication_types:
  - "1"
authors:
  - Jie Wei
  - Guanyu Hu
  - Xinyu Yang
  - Luu_Anh_Tuan
  - Yizhuo Dong
publication: "Interspeech"
publication_short: Interspeech
abstract: Speech emotion recognition has made significant progress in recent years, in which feature representation learning has been paid more attention, but discriminative emotional features extraction has remained unresolved. In this paper, we propose MDSCM - a Multi-attention based Depthwise Separable Convolutional Model for speech emotional feature extraction that can reduce the feature redundancy through separating spatialwise convolution and channel-wise convolution. MDSCM also enhances the feature discriminability by the multi-attention module that focuses on learning features with more emotional information. In addition, we propose an Audio-Visual Domain Adaptation Learning paradigm (AVDAL) to learn an audiovisual emotion-identity space. A shared audio-visual representation encoder is built to transfer the emotional knowledge learned from the visual domain to complement and enhance the emotional features that only extracted from speech. Domain classifier and emotion classifier are used for encoder training to reduce the mismatching of domain features, and enhance the discriminability of features for emotion recognition. The experimental results on the IEMOCAP dataset demonstrate that our proposed method outperforms other state-of-the-art speech emotion recognition systems, achieving 72.43% on weighted accuracy and 73.22% on unweighted accuracy.

draft: false
featured: false
tags:
  - Interspeech
date: '2022-10-01T00:00:00Z'
---
Link: https://www.isca-speech.org/archive/pdfs/interspeech_2022/wei22b_interspeech.pdf