---
title: Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion
Recognition
publication_types:
  - "1"
authors:
  - Jie Wei
  - Guanyu Hu
  - Xinyu Yang
  - Luu Anh Tuan
  - Yizhuo Dong
publication: "Interspeech"
publication_short: Interspeech
abstract: Speech emotion recognition has made significant progress in
recent years, in which feature representation learning has been
paid more attention, but discriminative emotional features extraction has remained unresolved. In this paper, we propose
MDSCM - a Multi-attention based Depthwise Separable Convolutional Model for speech emotional feature extraction that
can reduce the feature redundancy through separating spatialwise convolution and channel-wise convolution. MDSCM also
enhances the feature discriminability by the multi-attention
module that focuses on learning features with more emotional
information. In addition, we propose an Audio-Visual Domain
Adaptation Learning paradigm (AVDAL) to learn an audiovisual emotion-identity space. A shared audio-visual representation encoder is built to transfer the emotional knowledge
learned from the visual domain to complement and enhance the
emotional features that only extracted from speech. Domain
classifier and emotion classifier are used for encoder training
to reduce the mismatching of domain features, and enhance the
discriminability of features for emotion recognition. The experimental results on the IEMOCAP dataset demonstrate that
our proposed method outperforms other state-of-the-art speech
emotion recognition systems, achieving 72.43% on weighted
accuracy and 73.22% on unweighted accuracy.

draft: false
featured: false
tags:
  - Interspeech
image:
  filename: ""
  focal_point: Smart
  preview_only: false
date: 2023-03-2T08:29:00.179Z
---
https://www.isca-speech.org/archive/pdfs/interspeech_2022/wei22b_interspeech.pdf